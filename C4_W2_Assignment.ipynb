{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ec2e01",
   "metadata": {
    "colab_type": "text",
    "id": "7yuytuIllsv1"
   },
   "source": [
    "\n",
    "# Assignment 2: Transformer Summarizer\n",
    "\n",
    "Welcome to the second assignment of course 4. In this assignment you will explore summarization using the transformer model. Yes, you will implement the transformer decoder from scratch, but we will slowly walk you through it. There are many hints in this notebook so feel free to use them as needed. \n",
    "\n",
    "<img src = \"images/transformerNews.png\">\n",
    "\n",
    "## Important Note on Submission to the AutoGrader\n",
    "\n",
    "Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables.\n",
    "\n",
    "If you do any of the following, you will get something like, `Grader not found` (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don't remember the changes you have made, you can get a fresh copy of the assignment by following these [instructions](https://www.coursera.org/learn/attention-models-in-nlp/supplement/shBOS/how-to-refresh-your-workspace)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa202d8",
   "metadata": {
    "colab_type": "text",
    "id": "4-3lxSnXRWPx"
   },
   "source": [
    "## Outline\n",
    "\n",
    "- [Introduction](#0)\n",
    "- [Part 1: Importing the dataset](#1)\n",
    "    - [1.1 Encode & Decode helper functions](#1.1)\n",
    "    - [1.2 Defining parameters](#1.2)\n",
    "    - [1.3 Exploring the data](#1.3)\n",
    "- [Part 2: Summarization with transformer](#2)\n",
    "    - [2.1 Dot product attention](#2.1)\n",
    "        - [Exercise 01](#ex01)\n",
    "    - [2.2 Causal Attention](#2.2)\n",
    "        - [Exercise 02](#ex02)\n",
    "    - [2.3 Transformer decoder block](#2.3)\n",
    "        - [Exercise 03](#ex03)\n",
    "    - [2.4 Transformer Language model](#2.4)\n",
    "        - [Exercise 04](#ex04)\n",
    "- [Part 3: Training](#3)\n",
    "    - [3.1 Training the model](#3.1)\n",
    "        - [Exercise 05](#ex05)\n",
    "- [Part 4: Evaluation](#4)\n",
    "    - [4.1 Loading in a trained model](#4.1)\n",
    "- [Part 5: Testing with your own input](#5) \n",
    "    - [Exercise 6](#ex06)\n",
    "    - [5.1 Greedy decoding](#5.1)\n",
    "        - [Exercise 07](#ex07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32001217",
   "metadata": {
    "colab_type": "text",
    "id": "H4NlfEQhRWPy"
   },
   "source": [
    "<a name='0'></a>\n",
    "### Introduction\n",
    "\n",
    "Summarization is an important task in natural language processing and could be useful for a consumer enterprise. For example, bots can be used to scrape articles, summarize them, and then you can use sentiment analysis to identify the sentiment about certain stocks. Anyways who wants to read an article or a long email today, when you can build a transformer to summarize text for you. Let's get started, by completing this assignment you will learn to:  \n",
    "\n",
    "- Use built-in functions to preprocess your data\n",
    "- Implement DotProductAttention\n",
    "- Implement Causal Attention\n",
    "- Understand how attention works\n",
    "- Build the transformer model\n",
    "- Evaluate your model\n",
    "- Summarize an article\n",
    "\n",
    "As you can tell, this model is slightly different than the ones you have already implemented. This is heavily based on attention and does not rely on sequences, which allows for parallel computing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c58dc7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CChWzW-rEHVb",
    "outputId": "a0b3e98b-7fc6-492d-c8ad-3a263b54f670"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import w2_tests\n",
    "import numpy as np\n",
    "\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as jnp\n",
    "\n",
    "# to print the entire np array\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f756ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trax                         1.3.9\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list|grep trax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3552ae",
   "metadata": {
    "colab_type": "text",
    "id": "kEL2rvaHRWP4"
   },
   "source": [
    "<a name='1'></a>\n",
    "## Part 1: Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f77cd",
   "metadata": {},
   "source": [
    "Trax makes it easy to work with Tensorflow's datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a1a4a9",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VInmKSkhEhle"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7efc0d2f3950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7efc0d2f3950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7efc0d2f3950> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7efc0d2f3290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7efc0d2f3290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function TFDS.<locals>.select_from at 0x7efc0d2f3290> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "# This will download the dataset if no data_dir is specified.\n",
    "# Downloading and processing can take bit of time,\n",
    "# so we have the data already in 'data/' for you\n",
    "\n",
    "# Importing CNN/DailyMail articles dataset\n",
    "train_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                 data_dir='data/',\n",
    "                                 keys=('article', 'highlights'),\n",
    "                                 train=True)\n",
    "\n",
    "# This should be much faster as the data is downloaded already.\n",
    "eval_stream_fn = trax.data.TFDS('cnn_dailymail',\n",
    "                                data_dir='data/',\n",
    "                                keys=('article', 'highlights'),\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f62479",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "## 1.1 Tokenize & Detokenize helper functions\n",
    "\n",
    "Just like in the previous assignment, the cell above loads in the encoder for you. Given any data set, you have to be able to map words to their indices, and indices to their words. The inputs and outputs to your [Trax](https://github.com/google/trax) models are usually tensors of numbers where each number corresponds to a word. If you were to process your data manually, you would have to make use of the following: \n",
    "\n",
    "- <span style='color:blue'> word2Ind: </span> a dictionary mapping the word to its index.\n",
    "- <span style='color:blue'> ind2Word:</span> a dictionary mapping the index to its word.\n",
    "- <span style='color:blue'> word2Count:</span> a dictionary mapping the word to the number of times it appears. \n",
    "- <span style='color:blue'> num_words:</span> total number of words that have appeared. \n",
    "\n",
    "Since you have already implemented these in previous assignments of the specialization, we will provide you with helper functions that will do this for you. Run the cell below to get the following functions:\n",
    "\n",
    "- <span style='color:blue'> tokenize: </span> converts a text sentence to its corresponding token list (i.e. list of indices). Also converts words to subwords.\n",
    "- <span style='color:blue'> detokenize: </span> converts a token list to its corresponding sentence (i.e. string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd246b2c",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djTiSLcaNFGa"
   },
   "outputs": [],
   "source": [
    "def tokenize(input_str, EOS=1):\n",
    "    \"\"\"Input str to features dict, ready for inference\"\"\"\n",
    "  \n",
    "    # Use the trax.data.tokenize method. It takes streams and returns streams,\n",
    "    # we get around it by making a 1-element stream with `iter`.\n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_dir='vocab_dir/',\n",
    "                                      vocab_file='summarize32k.subword.subwords'))\n",
    "    \n",
    "    # Mark the end of the sentence with EOS\n",
    "    return list(inputs) + [EOS]\n",
    "\n",
    "def detokenize(integers):\n",
    "    \"\"\"List of ints to str\"\"\"\n",
    "  \n",
    "    s = trax.data.detokenize(integers,\n",
    "                             vocab_dir='vocab_dir/',\n",
    "                             vocab_file='summarize32k.subword.subwords')\n",
    "    \n",
    "    return wrapper.fill(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9ea15",
   "metadata": {
    "colab_type": "text",
    "id": "7WvhaFbCRWQS"
   },
   "source": [
    "<a name='1.2'></a>\n",
    "\n",
    "## 1.2 Preprocessing for Language Models: Concatenate It!\n",
    "\n",
    "This week you will use a language model -- Transformer Decoder -- to solve\n",
    "an input-output problem. As you know, language models only predict the next\n",
    "word, they have no notion of inputs. To create a single input suitable for\n",
    "a language model, we concatenate inputs with targets putting a separator\n",
    "in between. We also need to create a mask -- with 0s at inputs and 1s at targets -- so that the model is not penalized for mis-predicting the article and only focuses on the summary. See the preprocess function below for how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac8c8f7",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4rgPxYSRWQS"
   },
   "outputs": [],
   "source": [
    "# Special tokens\n",
    "SEP = 0 # Padding or separator token\n",
    "EOS = 1 # End of sentence token\n",
    "\n",
    "# Concatenate tokenized inputs and targets using 0 as separator.\n",
    "def preprocess(stream):\n",
    "    for (article, summary) in stream:\n",
    "        joint = np.array(list(article) + [EOS, SEP] + list(summary) + [EOS])\n",
    "        mask = [0] * (len(list(article)) + 2) + [1] * (len(list(summary)) + 1) # Accounting for EOS and SEP\n",
    "        yield joint, joint, np.array(mask)\n",
    "\n",
    "# You can combine a few data preprocessing steps into a pipeline like this.\n",
    "input_pipeline = trax.data.Serial(\n",
    "    # Tokenizes\n",
    "    trax.data.Tokenize(vocab_dir='vocab_dir/',\n",
    "                       vocab_file='summarize32k.subword.subwords'),\n",
    "    # Uses function defined above\n",
    "    preprocess,\n",
    "    # Filters out examples longer than 2048\n",
    "    trax.data.FilterByLength(2048)\n",
    ")\n",
    "\n",
    "# Apply preprocessing to data streams.\n",
    "train_stream = input_pipeline(train_stream_fn())\n",
    "eval_stream = input_pipeline(eval_stream_fn())\n",
    "\n",
    "train_input, train_target, train_mask = next(train_stream)\n",
    "\n",
    "assert sum((train_input - train_target)**2) == 0  # They are the same in Language Model (LM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7c4cd38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "uKFoGsUKSa_I",
    "outputId": "bc4d6634-d716-4311-d49c-1956bca2bc2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example mask:\n",
      "\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# prints mask, 0s on article, 1s on summary\n",
    "print(f'Single example mask:\\n\\n {train_mask}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04709c76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "S4uHyCkbSuUo",
    "outputId": "52845be8-f2fc-4803-bf7a-ed9725fe2bac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single example:\n",
      "\n",
      " Royal Mail were blasted today after they banned deliveries to more\n",
      "than 100 residents on a street following health and safety fears -\n",
      "over wonky paving slabs. Residents on the Victorian estate in Stoke-\n",
      "on-Trent, Staffordshire, are now forced to make a four-mile round trip\n",
      "collect their letters and parcels from a central depot. Royal Mail\n",
      "postmen are refusing to deliver to The Villas after it became a no-go\n",
      "zone due to 'a number of incidents involving colleagues falling and\n",
      "slipping'. Resident Catherine Burgass said the move by Royal Mail has\n",
      "'annoyed' all of the residents at The Villas . Yesterday home-owners\n",
      "on the road, which is not council-maintained as it is privately run by\n",
      "the residents' association, said their mail had been delivered without\n",
      "a problem for 150 years and slammed Royal Mail for not giving them any\n",
      "warning. The estate's residents' association has asked Royal Mail to\n",
      "identify which areas need repairing but say it would cost tens of\n",
      "thousands of pounds to tarmac the entire stretch. Elderly resident\n",
      "Alfred Poole, 79, branded the move 'barmy'. He said: 'Its health and\n",
      "safety gone mad. I'm an old man and if I can manage the paving slabs,\n",
      "I'm sure the postman can. 'I haven't got a car and I can't walk four\n",
      "miles to fetch my post. It's just ridiculous.' Royal Mail say they\n",
      "suspended deliveries to the street for health and safety reasons\n",
      "following 'incidents' Julian Wilshaw, 38, who lives with his partner\n",
      "Sarah and daughter Jessica, added: 'It came out of the blue. 'We had\n",
      "heard nothing at all about it and then the letter came through. 'We\n",
      "weren't even warned or informed there was an issue, so we could maybe\n",
      "try to help put any issues right. 'Royal Mail deliver to the likes of\n",
      "the Isle of Skye and the Hebrides - we're just The Villas, in Stoke.\n",
      "'My daughter has hip problems and we're waiting on a letter from the\n",
      "hospital. 'I have sympathy for the postmen if they've hurt themselves,\n",
      "but we don't know what the problem is, and Royal Mail have been\n",
      "delivering to us for years. 'It's a massive inconvenience for all the\n",
      "residents, especially the more elderly people living here.' A total of\n",
      "40 households and over 100 residents have been affected by the ban. Ms\n",
      "Burgass said the pavement has been in the same condition since she\n",
      "moved in 15 years ago . Martin Parker, 52, secretary of The Villas\n",
      "Residents' Association, said: 'The letter said one postman had injured\n",
      "themselves a month ago, and we were told it happened again a couple of\n",
      "days before the letter was sent. 'No-one had been consulted about any\n",
      "problems with the state of the pavements and I'm surprised they hadn't\n",
      "bothered to consult with us. 'It would have been sensible for them to\n",
      "speak to us, and it's bizarre Royal Mail has taken this rather extreme\n",
      "stance. 'The road is not slippery, our bins are emptied and post had\n",
      "been delivered here for a very long time. 'Luckily, I have a car and\n",
      "I'm physically fit, but there are quite a few elderly residents and\n",
      "there's the care home here too. 'Also, no-one seems to be able to\n",
      "contact the phone number attached with the letter.' Doug Burnham, 49,\n",
      "who has lived on The Villas for 15 years, added: 'It's a very\n",
      "difficult situation. I can't see how our neighbourhood differs from\n",
      "any other neighbourhood. Residents have slammed Royal Mail for not\n",
      "giving them any warning about the cancellation of the service .\n",
      "'Sweeping and cleaning is something we do as often as possible. We\n",
      "also tackle things like potholes, drainage issues and fly-tipping. 'We\n",
      "are open to discussions. 'We would be willing to pay if there were\n",
      "particularly problematic areas and we do that anyway, but obviously we\n",
      "couldn't do the entirety of the paving.' Resident Catherine Burgass,\n",
      "47, added: 'I was extremely surprised we had not had any prior\n",
      "warning. 'The pavement has been in the same condition since I moved in\n",
      "15 years ago. 'Everybody's annoyed about it.' A Royal Mail\n",
      "spokesperson said: 'We have taken the decision to suspend deliveries\n",
      "to a small number of addresses after a number of incidents involving\n",
      "colleagues falling and slipping. 'Our priority is the safety of our\n",
      "people and we apologise to customers.'<EOS><pad>RoyalMail say they\n",
      "have suspended deliveries after 'a number of incidents' which saw\n",
      "delivery drivers fall and slip on the private Victorian road . But\n",
      "residents say post has been delivered for 150 years with no issues .\n",
      "Residents claim they had no warning about the cancellation of the\n",
      "service . A total of 40 households and more than 100 residents are\n",
      "affected by the ban .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# prints: [Example][<EOS>][<pad>][Example Summary][<EOS>]\n",
    "print(f'Single example:\\n\\n {detokenize(train_input)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67360e32",
   "metadata": {
    "colab_type": "text",
    "id": "T4sDS1WIVaYG"
   },
   "source": [
    "<a name='1.3'></a>\n",
    "\n",
    "## 1.3 Batching with bucketing\n",
    "\n",
    "As in the previous week, we use bucketing to create batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40f76762",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oqj1NsbERWQX"
   },
   "outputs": [],
   "source": [
    "# Bucketing to create batched generators.\n",
    "\n",
    "# Buckets are defined in terms of boundaries and batch sizes.\n",
    "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
    "# So below, we'll take a batch of 16 sentences of length < 128 , 8 of length < 256,\n",
    "# 4 of length < 512. And so on. \n",
    "boundaries =  [128, 256,  512, 1024]\n",
    "batch_sizes = [16,    8,    4,    2, 1]\n",
    "\n",
    "# Create the streams.\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2690c64e",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6M5OA8QRWQb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1042)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every execution will result in generation of a different article\n",
    "# Try running this cell multiple times to see how the length of the examples affects the batch size\n",
    "input_batch, _, mask_batch = next(train_batch_stream)\n",
    "\n",
    "# Shape of the input_batch\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd3a26f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SjNOlljxTGuQ",
    "outputId": "9227c68c-6369-4ce8-8137-506c594f6ad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1562  1845     8 14607  5318    27 22512   494     2   964     2   864\n",
      "  1976  3873  1248   270     6  1054  4410   132   213 10995   583   527\n",
      "    28 18620  2648 18623   157  1353   973  1838 10835  3112   102 14850\n",
      "    28  8107   132 23677     4   527   281    32   446 20934     4     2\n",
      "   213  7802   415 20850     7     5   676 14380   127     3 21338 23450\n",
      "     5     2  1570     2    28   137     6   104  7619   527   213 22512\n",
      "   494  2713   676     2   229    43  3873  1248 13961  9236  8970 11599\n",
      " 23297   132   213   484   583   527  5942   821     2  1570     3 23450\n",
      "     5    80   228   186  1078  1689   213   281   571    88   226  1019\n",
      "   213  8107    76  1480  2867   229   137   237   527   213 20934     4\n",
      "    76   320  4182    15  1186  1838 16994     2   127  3306 10323 22991\n",
      "    92     2 14380  1019   213  7802   415 26319     4     3 23450     5\n",
      "  1353   973  4901   102 23056  3112     2 10323 22991    92   127  1133\n",
      " 27634     4   567   213    55    41   124   213 12683   126   186   539\n",
      "   527   285   844     2   125   450   103     7     5   285   532   132\n",
      "   213  1859  4617 27634   391 10323 22991    92   793 14607     3   821\n",
      "    80   779     2  9778     2  1874     2   527 24482  1907     2   964\n",
      "     2  3268  4620    17   320 23450     5    80  1186   186   127 23450\n",
      "     5   170    18    46   475   341 20934     4  1133 27634     4    13\n",
      "   358     7    26   483   134   973   166    22 23181   114 13355   130\n",
      "   293  4617 27634   391   127   821     2  1779     7     5    28  1646\n",
      " 13035  1019   213   986   748     3    69  1353   163  1127 10696    81\n",
      "   132   707 16735  1019   137    91    29    28 12499  2415  2112     2\n",
      "    22   169 15192   650     6   320     6   650  5421   320 16272   416\n",
      "   320  3573   186  5005     2    22   127     3  1312  3873   132   213\n",
      "   821   347   229  1497 16169     3  7796  3975  3790 15310 19922     2\n",
      "  1779   229  3873  1248 13961  9236  8970 11599 23297   186  8284 10990\n",
      "   147   527 13241  1017     3    69  1353   973   220   719    78   281\n",
      "   393    88   226 20934     4     3  1775  2021    18 18915    17    19\n",
      "  8272   320   213  3748     3     9    72   313  1435   398   635 22512\n",
      "   494   864  2021    76    38   313    76  1779    25   810   132   213\n",
      "   821  6901   186    18    46  1471    78 13961  9236  8970     2  1537\n",
      "  3541  1377     3     9 13151   229    43 14169   213  4287  1019  1424\n",
      "   544 10457     3  9778   821     2   213   779     2   127  3112    22\n",
      "  2976  2781  3748   214   193   527   213    54   290  2021  1133 27634\n",
      "     4  1473   122    22   141  4530    77   186   206  5778  1290     2\n",
      "   285     7     5  2754    22   170  1151  3873  1248     3    69   790\n",
      "     7    26  2481   130   293     7     5   583  4617 27634   391   821\n",
      "   127     3     9  7802   415   960  7047     7     5   797   127   824\n",
      "   984   285    92  3748    25  5159   214   213    54   290   166   669\n",
      " 27634     4   213   760   302    19   344  7084  2676   132   163 25528\n",
      "  1258    78   213   160   527    97  2021  4172 27634   391  5942   821\n",
      "     2    28  1570     6   104     6   292 18623   157  1248 26738 23529\n",
      "    28     2  1353 10996   691   864   145   163  7877  3191   484    65\n",
      "   186   710   409   390   272     3     9   347  7121  5512  1236   320\n",
      "   213   864  2154   527 22512   494     2  2685   393  1059 11382   527\n",
      "  5702  1562  1845     3  1191   146     2    72    54 11498   527 23181\n",
      "   554   809   213  1759   527   306   864    18  9643 25086     2   148\n",
      "  1730 12084  1173 20100     5   132   387     3  1193  8480  4152 10726\n",
      "  3051 14058   412   186  2907 13021     5  4924   381  1170   527  2137\n",
      " 21849 11519   846  2313  2754  2195   132   213   821   347     2 10726\n",
      "  3051 14058   412    23   127     3   821  4133  2644  6341     2 23028\n",
      " 16204     5     2  5150     4 16204     5     2   186  2333  6483  9249\n",
      "   186 12563  7580   400     2   213 13021     4     7     5   797   127\n",
      "     3     9  7802   415 19853    81  1490    15  2324   527   583   412\n",
      "    28 13638 10970   186   127    22   710   102   523    15 16872 26847\n",
      "  1407     2  2252   134  3630   320  7884    14     3 23450     5    40\n",
      "   133  1981  1242  1248   821    76  1457    22  1669   412    28   669\n",
      " 27634     4 18623 10607    81 27634    76   102   864   625    28   922\n",
      "  2685    28 18623   157   996   132  1081  3778   186 19277    78 20623\n",
      "     5   527 19363    17  3040     2 10726  3051 14058   412   127  1133\n",
      " 27634     4    69 18283    15 20490     5   320  5942   821   186    22\n",
      "   127   948   267   172   130 20490     4    98  1333    41     7   165\n",
      "  1269   320  2127   320   242    33    61 22137   669 27634   391 10726\n",
      "  3051 14058   412   127     2   354   669 27634     4   242 27634   391\n",
      "  1382   527   213   444  7648  4854   360     3     9   960  7047   127\n",
      " 23450     5    80  2751  1353 19357 12965   217   186   669 27634     4\n",
      "    19  9434   186  2554 27634   391   213   218  1133 27634     4 23450\n",
      "     5    40   320   288   285    22  1353  2167    28  1359  4872  5942\n",
      "   821 19042  1019    15   177   186  1353 10655   320   211   463  1838\n",
      "   163  4574  1976  1779  1353   416   320   117   242   134    61 22137\n",
      " 27634   391 10726  3051 14058   412   127     3  3790 15310 19922  2362\n",
      "   809   213  1610   272     3    69   229  5754   527   354 13241  1017\n",
      "  7511    22 12365 24810    17   186  2530   821     2   176   354   213\n",
      "   979   250   527    15 11818    81   320  1205   213  7599    78   213\n",
      "   570   186   882  1223   450   192   213   157  1353 13082    17   320\n",
      "   213   927   691    54  2021     3   348   285   340     2   821  1353\n",
      "   454    92 21708   478  3949     2  7134   285    22  1353   669 27634\n",
      "     4   246   186  5585  5973  4617 27634   391   213 13021     4     7\n",
      "     5   797   127     3 14607     7     5 25000   551 22951  3625   320\n",
      "   824   648    10     1     0 19088    11  5440   527  6402  4410  7599\n",
      "  3520   320  1186  1838 10835 16346 27439  6774  1628  2445   186  1078\n",
      "   527  3797 21338 23450     5   399  3606   750   320   440   134 16346\n",
      " 27439  6774  1628 23450     5   186    15  5478  1153   281   571    88\n",
      "   226  1019    28  8107   132 23677     4   527 20934     4 16346 27439\n",
      "  6774  1628  1902  1976  3873   132 18623   157     7     5   583   229\n",
      "   440    78   281   393    88   226 20934     4  2104     1]\n"
     ]
    }
   ],
   "source": [
    "# print corresponding integer values\n",
    "print(input_batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b718a8",
   "metadata": {
    "colab_type": "text",
    "id": "GD-72TENV2Jk"
   },
   "source": [
    "Things to notice:\n",
    " - First we see the corresponding values of the words.\n",
    " - The first 1, which represents the `<EOS>` tag of the article.\n",
    " - Followed by a 0, which represents a `<pad>` tag.\n",
    " - After the first 0 (`<pad>` tag) the corresponding values are of the words that are used for the summary of the article.\n",
    " - The second 1 represents the `<EOS>` tag for the summary.\n",
    " - All the trailing 0s represent `<pad>` tags which are appended to maintain consistent length (If you don't see them then it would mean it is already of max length)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2007cc17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Bu05ZwbWTE6P",
    "outputId": "3d455bd7-e343-4c25-a467-572d2abd837f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article:\n",
      "\n",
      " Los Angeles (CNN) -- A Fullerton, California, police officer charged\n",
      "with second-degree murder in the beating death of a mentally ill\n",
      "homeless man was released from jail Thursday after posting a bond in\n",
      "lieu of $1 million bail, the Orange County Sheriff's Department\n",
      "spokesman said. Manuel Ramos, 37, a 10-year veteran of the Fullerton\n",
      "Police Department, is also charged with involuntary manslaughter in\n",
      "the July death of Kelly Thomas, 37. Ramos' family and friends raised\n",
      "the $100,000 for the bond -- which typically is 10% of the bail -- to\n",
      "secure his release from custody, said Jim Amormino, spokesman for the\n",
      "Orange County sheriff. Ramos was released shortly after midnight\n",
      "Thursday, Amormino said. \"By the time they do the paperwork and things\n",
      "of that nature, many times it's that late in the morning,\" Amormino\n",
      "told CNN. Thomas' father, Ron, 55, of Cypress, California, objected to\n",
      "Ramos' release and said Ramos should have been held without bail. \"I\n",
      "don't want him released because he brutally murdered my son,\" said\n",
      "Thomas, who's a safety consultant for the construction industry. He\n",
      "was an Army Ranger in special ops for 10 years; a martial arts master,\n",
      "he now teaches hand-to-hand combat to Marines going to Iraq and\n",
      "Afghanistan, he said. Also charged in the Thomas case is Cpl. Jay\n",
      "Patrick Cicinelli, who is charged with involuntary manslaughter and\n",
      "felony use of excessive force. He was released last week on $25,000\n",
      "bail. Both officers have pleaded not guilty to the charges. The two\n",
      "men are among six Fullerton police officers -- all men -- who were\n",
      "involved in the Thomas arrest and have been placed on involuntary,\n",
      "paid administrative leave. The FBI is also investigating the incident\n",
      "for civil rights violations. Ron Thomas, the father, said Thursday he\n",
      "wants criminal charges against each of the other four officers. \"Even\n",
      "if he just stood there and did absolutely nothing, that's what he\n",
      "should be charged with. He didn't prevent my son's death,\" Thomas\n",
      "said. The Orange County district attorney's office said this month\n",
      "that no charges were filed against the other four because \"the\n",
      "evidence does not show knowing participation in an unlawful act on the\n",
      "part of these officers.\" Kelly Thomas, a 37-year-old homeless man with\n",
      "schizophrenia, was beaten by police during an altercation July 5 and\n",
      "died five days later. The case drew widespread attention to the police\n",
      "department of Fullerton, about 25 miles southeast of downtown Los\n",
      "Angeles. Since then, two other allegations of brutality at the hands\n",
      "of city police have surfaced, both regarding unrelated arrests in\n",
      "2010. District Attorney Tony Rackauckas and fellow prosecutors viewed\n",
      "16 minutes of bus depot surveillance video showing what happened in\n",
      "the Thomas case, Rackauckas has said. Thomas suffered brain injuries,\n",
      "facial fractures, rib fractures, and extensive bruising and abrasions,\n",
      "the prosecutor's office said. The Orange County coroner listed his\n",
      "manner of death as a homicide and said he died after having his chest\n",
      "compressed, leaving him unable to breathe. Ramos had made initial\n",
      "contact with Thomas -- whom he knew as a \"homeless drifter\" -- after\n",
      "police received a call about a homeless man looking in car windows and\n",
      "pulling on handles of parked cars, Rackauckas said. \"He lifted his\n",
      "fists to Kelly Thomas and he said, 'You see my fist? Now they're\n",
      "getting to ready to F you up,' \" Rackauckas said, using \"F\" instead of\n",
      "the full profanity. The district attorney said Ramos' conduct was\n",
      "unacceptable and \"not protecting and serving\" the public. \"Ramos had\n",
      "to know that he was creating a situation where Kelly Thomas feared for\n",
      "his life and was struggling to get away from an armed officer who was\n",
      "going to 'F him up,'\" Rackauckas said. Cicinelli arrived at the scene\n",
      "later. He is accused of using excessive force when he allegedly\n",
      "assaulted and beat Thomas, including using the front end of his Taser\n",
      "to hit the victim on the head and face eight times while the man was\n",
      "pinned to the ground by other officers. At that point, Thomas was\n",
      "making no audible sounds, indicating that he was \"down and seriously\n",
      "injured,\" the prosecutor's office said. CNN's Stella Chan contributed\n",
      "to this report.<EOS><pad>NEW: Father of alleged murder victim objects\n",
      "to release from jail . Family and friends of Officer Manuel Ramos help\n",
      "raise money to free him . Ramos and his supporters pay $100,000 for a\n",
      "bond in lieu of bail . Second officer charged in homeless man's death\n",
      "is free on $25,000 bail .<EOS>\n"
     ]
    }
   ],
   "source": [
    "# print the article and its summary\n",
    "print('Article:\\n\\n', detokenize(input_batch[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7ac15",
   "metadata": {
    "colab_type": "text",
    "id": "aNFVhgHoncGm"
   },
   "source": [
    "You can see that the data has the following structure:\n",
    "- <span style='color:blue'> [Article] </span> -> `<EOS>` -> `<pad>` -> <span style='color:blue'> [Article Summary] </span> -> `<EOS>` -> (possibly) multiple `<pad>`\n",
    "\n",
    "The loss is taken only on the summary using cross_entropy as loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b226fc",
   "metadata": {
    "colab_type": "text",
    "id": "Un8NHIRoj-1W"
   },
   "source": [
    "<a name='2'></a>\n",
    "# Part 2: Summarization with transformer\n",
    "\n",
    "Now that we have given you the data generator and have handled the preprocessing for you, it is time for you to build your own model. We saved you some time because we know you have already preprocessed data before in this specialization, so we would rather you spend your time doing the next steps. \n",
    "\n",
    "You will be implementing the attention from scratch and then using it in your transformer model. Concretely, you will understand how attention works, how you use it to connect the encoder and the decoder.\n",
    "\n",
    "<img src=\"images/transformer_decoder_zoomin.png\">\n",
    "\n",
    "<a name='2.1'></a>\n",
    "## 2.1 Dot product attention \n",
    "\n",
    "Now you will implement dot product attention which takes in a query, key, value, and a mask. It returns the output. \n",
    "\n",
    "<img src =\"images/dotproduct.png\">\n",
    "\n",
    "\n",
    "Here are some helper functions that will help you create tensors and display useful information:\n",
    "   - `create_tensor`  creates a `jax numpy array` from a list of lists.\n",
    "   - `display_tensor` prints out the shape and the actual tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b2a5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(t):\n",
    "    \"\"\"Create tensor from list of lists\"\"\"\n",
    "    return jnp.array(t)\n",
    "\n",
    "\n",
    "def display_tensor(t, name):\n",
    "    \"\"\"Display shape and tensor\"\"\"\n",
    "    print(f'{name} shape: {t.shape}\\n')\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c021356",
   "metadata": {},
   "source": [
    "Before implementing it yourself, you can play around with a toy example of `dot product attention` without the softmax  operation. Technically it would not be `dot product attention` without the softmax but this is done to avoid giving away too much of the answer and the idea is to display these tensors to give you a sense of how they look like.\n",
    "\n",
    "The formula for attention is this one:\n",
    "\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n",
    "$$\n",
    "\n",
    "$d_{k}$ stands for the dimension of queries and keys.\n",
    "\n",
    "The `query`, `key`, `value` and `mask` vectors are provided for this example.\n",
    "\n",
    "Notice that the masking is done using very negative values that will yield a similar effect to using $-\\infty $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b39ebe0d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "_0x0HJXwRWQk",
    "outputId": "d6d78a8e-e3cc-47af-9584-2bdcdfcca0cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "key shape: (2, 3)\n",
      "\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "value shape: (2, 3)\n",
      "\n",
      "[[0 1 0]\n",
      " [1 0 1]]\n",
      "\n",
      "mask shape: (2, 2)\n",
      "\n",
      "[[ 0.e+00  0.e+00]\n",
      " [-1.e+09  0.e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = create_tensor([[1, 0, 0], [0, 1, 0]])\n",
    "display_tensor(q, 'query')\n",
    "k = create_tensor([[1, 2, 3], [4, 5, 6]])\n",
    "display_tensor(k, 'key')\n",
    "v = create_tensor([[0, 1, 0], [1, 0, 1]])\n",
    "display_tensor(v, 'value')\n",
    "m = create_tensor([[0, 0], [-1e9, 0]])\n",
    "display_tensor(m, 'mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff81d1",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "query shape: (2, 3)\n",
    "\n",
    "[[1 0 0]\n",
    " [0 1 0]]\n",
    "\n",
    "key shape: (2, 3)\n",
    "\n",
    "[[1 2 3]\n",
    " [4 5 6]]\n",
    "\n",
    "value shape: (2, 3)\n",
    "\n",
    "[[0 1 0]\n",
    " [1 0 1]]\n",
    "\n",
    "mask shape: (2, 2)\n",
    "\n",
    "[[ 0.e+00  0.e+00]\n",
    " [-1.e+09  0.e+00]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2977c81",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "kVR9u4faRWQo",
    "outputId": "f01ea4ca-4152-4b54-b76a-e4b5917ae2b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query dot key shape: (2, 2)\n",
      "\n",
      "[[0.57735026 2.309401  ]\n",
      " [1.1547005  2.8867514 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_dot_k = q @ k.T / jnp.sqrt(3)\n",
    "display_tensor(q_dot_k, 'query dot key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe3420",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "query dot key shape: (2, 2)\n",
    "\n",
    "[[0.57735026 2.309401  ]\n",
    " [1.1547005  2.8867514 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4904dd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked query dot key shape: (2, 2)\n",
      "\n",
      "[[ 5.7735026e-01  2.3094010e+00]\n",
      " [-1.0000000e+09  2.8867514e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "masked = q_dot_k + m\n",
    "display_tensor(masked, 'masked query dot key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a8bc1",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "masked query dot key shape: (2, 2)\n",
    "\n",
    "[[ 5.7735026e-01  2.3094010e+00]\n",
    " [-1.0000000e+09  2.8867514e+00]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "818aa6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masked query dot key dot value shape: (2, 3)\n",
      "\n",
      "[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n",
      " [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(masked @ v, 'masked query dot key dot value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a56ee4",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "masked query dot key dot value shape: (2, 3)\n",
    "\n",
    "[[ 2.3094010e+00  5.7735026e-01  2.3094010e+00]\n",
    " [ 2.8867514e+00 -1.0000000e+09  2.8867514e+00]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d03a4",
   "metadata": {},
   "source": [
    "In order to use the previous dummy tensors to test some of the graded functions, a batch dimension should be added to them so they mimic the shape of real-life examples. The mask is also replaced by a version of it that resembles the one that is used by trax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9563543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n",
      "key with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[1 2 3]\n",
      "  [4 5 6]]]\n",
      "\n",
      "value with batch dim shape: (1, 2, 3)\n",
      "\n",
      "[[[0 1 0]\n",
      "  [1 0 1]]]\n",
      "\n",
      "boolean mask shape: (2, 2)\n",
      "\n",
      "[[ True  True]\n",
      " [False  True]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_with_batch = q[None,:]\n",
    "display_tensor(q_with_batch, 'query with batch dim')\n",
    "k_with_batch = k[None,:]\n",
    "display_tensor(k_with_batch, 'key with batch dim')\n",
    "v_with_batch = v[None,:]\n",
    "display_tensor(v_with_batch, 'value with batch dim')\n",
    "m_bool = create_tensor([[True, True], [False, True]])\n",
    "display_tensor(m_bool, 'boolean mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eee6eb",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "query with batch dim shape: (1, 2, 3)\n",
    "\n",
    "[[[1 0 0]\n",
    "  [0 1 0]]]\n",
    "\n",
    "key with batch dim shape: (1, 2, 3)\n",
    "\n",
    "[[[1 2 3]\n",
    "  [4 5 6]]]\n",
    "\n",
    "value with batch dim shape: (1, 2, 3)\n",
    "\n",
    "[[[0 1 0]\n",
    "  [1 0 1]]]\n",
    "\n",
    "boolean mask shape: (2, 2)\n",
    "\n",
    "[[ True  True]\n",
    " [False  True]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c4ba5",
   "metadata": {},
   "source": [
    "<a name='ex01'></a>\n",
    "### Exercise 01\n",
    "\n",
    "**Instructions:** Implement the dot product attention. Concretely, implement the following equation\n",
    "\n",
    "\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{1}\\\n",
    "$$\n",
    "\n",
    "$Q$ - query, \n",
    "$K$ - key, \n",
    "$V$ - values, \n",
    "$M$ - mask, \n",
    "${d_k}$ - depth/dimension of the queries and keys (used for scaling down)\n",
    "\n",
    "You can implement this formula either by `trax` numpy (trax.math.numpy) or regular `numpy` but it is recommended to use `jnp`.\n",
    "\n",
    "Something to take into consideration is that within trax, the masks are tensors of `True/False` values not 0's and $-\\infty$ as in the previous example. Within the graded function don't think of applying the mask by summing up matrices, instead use `jnp.where()` and treat the **mask as a tensor of boolean values with `False` for values that need to be masked and True for the ones that don't.**\n",
    "\n",
    "Also take into account that the real tensors are far more complex than the toy ones you just played with. Because of this avoid using shortened operations such as `@` for dot product or `.T` for transposing. Use `jnp.matmul()` and `jnp.swapaxes()` instead.\n",
    "\n",
    "This is the self-attention block for the transformer decoder. Good luck!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5414224e",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSauPt0NUl_o"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: DotProductAttention\n",
    "def DotProductAttention(query, key, value, mask):\n",
    "    \"\"\"Dot product self-attention.\n",
    "    Args:\n",
    "        query (jax.interpreters.xla.DeviceArray): array of query representations with shape (L_q by d)\n",
    "        key (jax.interpreters.xla.DeviceArray): array of key representations with shape (L_k by d)\n",
    "        value (jax.interpreters.xla.DeviceArray): array of value representations with shape (L_k by d) where L_v = L_k\n",
    "        mask (jax.interpreters.xla.DeviceArray): attention-mask, gates attention with shape (L_q by L_k)\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Self-attention array for q, k, v arrays. (L_q by L_k)\n",
    "    \"\"\"\n",
    "\n",
    "    assert query.shape[-1] == key.shape[-1] == value.shape[-1], \"Embedding dimensions of q, k, v aren't all the same\"\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    # Save depth/dimension of the query embedding for scaling down the dot product\n",
    "    depth = query.shape[-1]\n",
    "\n",
    "    # Calculate scaled query key dot product according to formula above\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.swapaxes.html\n",
    "    dots = jnp.matmul(query, jnp.swapaxes(key, -1, -2)) / jnp.sqrt(depth)\n",
    "    \n",
    "    # Apply the mask\n",
    "    if mask is not None: # You do not need to replace the 'None' on this line\n",
    "        dots = jnp.where(mask, x=dots, y=jnp.full_like(dots, -1e9)) # when condition is True, yield X otherwise yield y\n",
    "    \n",
    "    # Softmax formula implementation\n",
    "    # Use trax.fastmath.logsumexp of masked_qkT to avoid underflow by division by large numbers\n",
    "    # Note: softmax = e^(dots - logaddexp(dots)) = E^dots / sumexp(dots)\n",
    "    logsumexp = trax.fastmath.logsumexp(dots, axis=-1, keepdims=True)\n",
    "\n",
    "    # Take exponential of dots minus logsumexp to get softmax\n",
    "    # Use jnp.exp()\n",
    "    dots = jnp.exp(dots-logsumexp)\n",
    "\n",
    "    # Multiply dots by value to get self-attention\n",
    "    # Use jnp.matmul()\n",
    "    attention = jnp.matmul(dots, value)\n",
    "\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90985c23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8o0K7VWKRWQw",
    "outputId": "1c51af3a-5f11-480f-b33b-419072d8298c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],\n",
       "              [1.        , 0.        , 1.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DotProductAttention(q_with_batch, k_with_batch, v_with_batch, m_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aa5cfd",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "DeviceArray([[[0.8496746 , 0.15032545, 0.8496746 ],\n",
    "              [1.        , 0.        , 1.        ]]], dtype=float32)\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2798065d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "# test DotProductAttention\n",
    "w2_tests.test_DotProductAttention(DotProductAttention)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9226dbdc",
   "metadata": {
    "colab_type": "text",
    "id": "2y2PSiLVRWQ2"
   },
   "source": [
    "<a name='2.2'></a>\n",
    "\n",
    "## 2.2 Causal Attention\n",
    "\n",
    "Now you are going to implement causal attention: multi-headed attention with a mask to attend only to words that occurred before. \n",
    "\n",
    "<img src = \"images/causal.png\">\n",
    "\n",
    "In the image above, a word can see everything that is before it, but not what is after it. To implement causal attention, you will have to transform vectors and do many reshapes. You will need to implement the functions below.\n",
    "\n",
    "\n",
    "<a name='ex02'></a>\n",
    "### Exercise 02\n",
    "\n",
    "Implement the following functions that will be needed for Causal Attention:\n",
    "\n",
    "- <span style='color:blue'> compute_attention_heads </span>: Gets an input $x$ of dimension (n_batch, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (n_batch $\\times$ n_heads, seqlen, d_head). Used to apply attention.\n",
    "- <span style='color:blue'> dot_product_self_attention </span>: Creates a mask matrix with `False` values above the diagonal and `True` values below and calls DotProductAttention which implements dot product self attention.\n",
    "- <span style='color:blue'> compute_attention_output </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (n_batch, seqlen, n_heads $\\times$ d_head). These operations concatenate (stack/merge) the heads. \n",
    "\n",
    "Next there are some toy tensors which may serve to give you an idea of the data shapes and opperations involved in Causal Attention. They are also useful to test out your functions! \n",
    "  \n",
    "    \n",
    "## Multi-head causal attention\n",
    "\n",
    "The layers and array dimensions involved in multi-head causal attention (which looks at previous words in the input text) are summarized in the figure below: \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "`tl.CausalAttention()` does all of this for you! You might be wondering, though, whether you need to pass in your input text 3 times, since for causal attention, the queries Q, keys K, and values V all come from the same source. Fortunately, `tl.CausalAttention()` handles this as well by making use of the [`tl.Branch()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators) combinator layer. In general, each branch within a `tl.Branch()` layer performs parallel operations on copies of the layer's inputs. For causal attention, each branch (representing Q, K, and V) applies a linear transformation (i.e. a dense layer without a subsequent activation) to its copy of the input, then splits that result into heads. You can see the syntax for this in the screenshot from the `trax.layers.attention.py` [source code](https://github.com/google/trax/blob/master/trax/layers/attention.py) below: \n",
    "\n",
    "<img src=\"images/use-of-tl-Branch-in-tl-CausalAttention.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "684788b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "VRH67YcrRWQ3",
    "outputId": "847a9416-877a-4246-c738-0eacdf46de59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query matrix (2D tensor) shape: (2, 3)\n",
      "\n",
      "[[1 0 0]\n",
      " [0 1 0]]\n",
      "\n",
      "batch of two (multi-head) collections of query matrices (4D tensor) shape: (2, 2, 2, 3)\n",
      "\n",
      "[[[[1 0 0]\n",
      "   [0 1 0]]\n",
      "\n",
      "  [[1 0 0]\n",
      "   [0 1 0]]]\n",
      "\n",
      "\n",
      " [[[1 0 0]\n",
      "   [0 1 0]]\n",
      "\n",
      "  [[1 0 0]\n",
      "   [0 1 0]]]]\n",
      "\n",
      "one batch of concatenated heads of query matrices (3d tensor) shape: (1, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n",
      "three batches of concatenated heads of query matrices (3d tensor) shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor2d = create_tensor(q)\n",
    "display_tensor(tensor2d, 'query matrix (2D tensor)')\n",
    "\n",
    "tensor4d2b = create_tensor([[q, q], [q, q]])\n",
    "display_tensor(tensor4d2b, 'batch of two (multi-head) collections of query matrices (4D tensor)')\n",
    "\n",
    "tensor3dc = create_tensor([jnp.concatenate([q, q], axis = -1)])\n",
    "display_tensor(tensor3dc, 'one batch of concatenated heads of query matrices (3d tensor)')\n",
    "\n",
    "tensor3dc3b = create_tensor([jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1), jnp.concatenate([q, q], axis = -1)])\n",
    "display_tensor(tensor3dc3b, 'three batches of concatenated heads of query matrices (3d tensor)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d16a99",
   "metadata": {},
   "source": [
    "It is important to know that the following 3 functions would normally be defined within the `CausalAttention` function further below. \n",
    "\n",
    "However this makes these functions harder to test. Because of this, these functions are shown individually using a `closure` (when necessary) that simulates them being inside of the `CausalAttention` function. This is done because they rely on some variables that can be accessed from within `CausalAttention`.\n",
    "\n",
    "### Support Functions\n",
    "\n",
    "<span style='color:blue'> compute_attention_heads </span>: Gets an input $x$ of dimension (n_batch, seqlen, n_heads $\\times$ d_head) and splits the last (depth) dimension and stacks it to the zeroth dimension to allow matrix multiplication (n_batch $\\times$ n_heads, seqlen, d_head).\n",
    "\n",
    "**For the closures you only have to fill the inner function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc768855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_attention_heads_closure\n",
    "def compute_attention_heads_closure(n_heads, d_head):\n",
    "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
    "    Args:\n",
    "        d_head (int):  dimensionality of heads\n",
    "        n_heads (int): number of attention heads\n",
    "    Returns:\n",
    "        function: compute_attention_heads function\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_attention_heads(x):\n",
    "        \"\"\" Compute the attention heads.\n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape (n_batch, seqlen, n_heads X d_head).\n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (n_batch X n_heads, seqlen, d_head).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        # (REPLACE INSTANCES OF 'None' WITH YOUR CODE)\n",
    "        \n",
    "        # Size of the x's batch dimension\n",
    "        batch_size = x.shape[0]\n",
    "        # Length of the sequence\n",
    "        # Should be size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[1]\n",
    "        # Reshape x using jnp.reshape()\n",
    "#         print(x.shape)\n",
    "        # n_batch, seqlen, n_heads*d_head -> n_batch, seqlen, n_heads, d_head\n",
    "        x = jnp.reshape(x, (batch_size, seqlen, n_heads, d_head))\n",
    "        # Transpose x using jnp.transpose()\n",
    "        # n_batch, seqlen, n_heads, d_head -> n_batch, n_heads, seqlen, d_head\n",
    "        # Note that the values within the tuple are the indexes of the dimensions of x and you must rearrange them\n",
    "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
    "        # Reshape x using jnp.reshape()\n",
    "        # n_batch, n_heads, seqlen, d_head -> n_batch*n_heads, seqlen, d_head\n",
    "        x = jnp.reshape(x, (batch_size*n_heads, seqlen, d_head))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return x\n",
    "    return compute_attention_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35b2ba26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n",
      "output tensor shape: (6, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(tensor3dc3b, \"input tensor\")\n",
    "result_cah = compute_attention_heads_closure(n_heads=2,d_head=3)(tensor3dc3b)\n",
    "display_tensor(result_cah, \"output tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc43ee54",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "input tensor shape: (3, 2, 6)\n",
    "\n",
    "[[[1 0 0 1 0 0]\n",
    "  [0 1 0 0 1 0]]\n",
    "\n",
    " [[1 0 0 1 0 0]\n",
    "  [0 1 0 0 1 0]]\n",
    "\n",
    " [[1 0 0 1 0 0]\n",
    "  [0 1 0 0 1 0]]]\n",
    "\n",
    "output tensor shape: (6, 2, 3)\n",
    "\n",
    "[[[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08964199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 6)\n",
      "(3, 2, 6)\n",
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "# test compute_attention_heads_closure\n",
    "w2_tests.test_compute_attention_heads_closure(compute_attention_heads_closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f545f",
   "metadata": {},
   "source": [
    "<span style='color:blue'> dot_product_self_attention </span>: Creates a mask matrix with `False` values above the diagonal and `True` values below and calls DotProductAttention which implements dot product self attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "858468e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: dot_product_self_attention\n",
    "def dot_product_self_attention(q, k, v):\n",
    "    \"\"\" Masked dot product self attention.\n",
    "    Args:\n",
    "        q (jax.interpreters.xla.DeviceArray): queries.\n",
    "        k (jax.interpreters.xla.DeviceArray): keys.\n",
    "        v (jax.interpreters.xla.DeviceArray): values.\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: masked dot product self attention tensor.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
    "    \n",
    "    # Hint: mask size should be equal to L_q. Remember that q has shape (batch_size, L_q, d)\n",
    "    mask_size = q.shape[-2]\n",
    "\n",
    "\n",
    "    # Creates a matrix with ones below the diagonal and 0s above. It should have shape (1, mask_size, mask_size)\n",
    "    # Notice that 1's and 0's get casted to True/False by setting dtype to jnp.bool_\n",
    "    # Use jnp.tril() - Lower triangle of an array and jnp.ones()\n",
    "    mask = jnp.tril(np.ones((1, mask_size, mask_size), dtype=np.bool_), k=0)  \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return DotProductAttention(q, k, v, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fa669f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[0.        , 1.        , 0.        ],\n",
       "              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_self_attention(q_with_batch, k_with_batch, v_with_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5006f8",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "DeviceArray([[[0.        , 1.        , 0.        ],\n",
    "              [0.8496746 , 0.15032543, 0.8496746 ]]], dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fe72314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "# test dot_product_self_attention\n",
    "w2_tests.test_dot_product_self_attention(dot_product_self_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051d77f2",
   "metadata": {},
   "source": [
    "<span style='color:blue'> compute_attention_output </span>: Undoes compute_attention_heads by splitting first (vertical) dimension and stacking in the last (depth) dimension (n_batch, seqlen, n_heads $\\times$ d_head). These operations concatenate (stack/merge) the heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "361aad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: compute_attention_output_closure\n",
    "def compute_attention_output_closure(n_heads, d_head):\n",
    "    \"\"\" Function that simulates environment inside CausalAttention function.\n",
    "    Args:\n",
    "        d_head (int):  dimensionality of heads\n",
    "        n_heads (int): number of attention heads\n",
    "    Returns:\n",
    "        function: compute_attention_output function\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_attention_output(x):\n",
    "        \"\"\" Compute the attention output.\n",
    "        Args:\n",
    "            x (jax.interpreters.xla.DeviceArray): tensor with shape (n_batch X n_heads, seqlen, d_head).\n",
    "        Returns:\n",
    "            jax.interpreters.xla.DeviceArray: reshaped tensor with shape (n_batch, seqlen, n_heads X d_head).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "        \n",
    "        # Length of the sequence\n",
    "        # Should be size of x's first dimension without counting the batch dim\n",
    "        seqlen = x.shape[1]\n",
    "        # Reshape x using jnp.reshape() to shape (n_batch, n_heads, seqlen, d_head)\n",
    "        x = jnp.reshape(x, (-1, n_heads, seqlen, d_head))\n",
    "        # Transpose x using jnp.transpose() to shape (n_batch, seqlen, n_heads, d_head)\n",
    "        x = jnp.transpose(x, (0, 2, 1, 3))\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Reshape to allow to concatenate the heads\n",
    "        return jnp.reshape(x, (-1, seqlen, n_heads * d_head))\n",
    "    return compute_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db2f382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor shape: (6, 2, 3)\n",
      "\n",
      "[[[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]\n",
      "\n",
      " [[1 0 0]\n",
      "  [0 1 0]]]\n",
      "\n",
      "output tensor shape: (3, 2, 6)\n",
      "\n",
      "[[[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]\n",
      "\n",
      " [[1 0 0 1 0 0]\n",
      "  [0 1 0 0 1 0]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(result_cah, \"input tensor\")\n",
    "result_cao = compute_attention_output_closure(2,3)(result_cah)\n",
    "display_tensor(result_cao, \"output tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0916a",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "input tensor shape: (6, 2, 3)\n",
    "\n",
    "[[[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]\n",
    "\n",
    " [[1 0 0]\n",
    "  [0 1 0]]]\n",
    "\n",
    "output tensor shape: (3, 2, 6)\n",
    "\n",
    "[[[1 0 0 1 0 0]\n",
    "  [0 1 0 0 1 0]]\n",
    "\n",
    " [[1 0 0 1 0 0]\n",
    "  [0 1 0 0 1 0]]\n",
    "\n",
    " [[1 0 0 1 0 0]\n",
    "  [0 1 0 0 1 0]]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "936b7e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "# test compute_attention_output_closure\n",
    "w2_tests.test_compute_attention_output_closure(compute_attention_output_closure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490769f",
   "metadata": {},
   "source": [
    "### Causal Attention Function\n",
    "\n",
    "Now it is time for you to put everything together within the `CausalAttention` or Masked multi-head attention function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cfdc7d",
   "metadata": {},
   "source": [
    "<img src = \"images/masked-attention.png\"> \n",
    "\n",
    "**Instructions:** Implement the causal attention.\n",
    "Your model returns the causal attention through a $tl.Serial$ with the following:\n",
    "\n",
    "- <span style='color:blue'> [tl.Branch](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Branch) </span>: consisting of 3 [tl.Dense(d_feature), ComputeAttentionHeads] to account for the queries, keys, and values.\n",
    "- <span style='color:blue'> [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)</span>: Takes in dot_product_self_attention function and uses it to compute the dot product using $Q$, $K$, $V$.\n",
    "- <span style='color:blue'> [tl.Fn](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)</span>: Takes in compute_attention_output_closure to allow for parallel computing.\n",
    "- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)</span>: Final Dense layer, with dimension `d_feature`.\n",
    "\n",
    "Remember that in order for trax to properly handle the functions you just defined, they need to be added as layers using the [`tl.Fn()`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af1ff4c7",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9Adn6DtRWRG"
   },
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED FUNCTION: CausalAttention\n",
    "def CausalAttention(d_feature, \n",
    "                    n_heads, \n",
    "                    compute_attention_heads_closure=compute_attention_heads_closure,\n",
    "                    dot_product_self_attention=dot_product_self_attention,\n",
    "                    compute_attention_output_closure=compute_attention_output_closure,\n",
    "                    mode='train'):\n",
    "    \"\"\"Transformer-style multi-headed causal attention.\n",
    "\n",
    "    Args:\n",
    "        d_feature (int):  dimensionality of feature embedding.\n",
    "        n_heads (int): number of attention heads.\n",
    "        compute_attention_heads_closure (function): Closure around compute_attention heads.\n",
    "        dot_product_self_attention (function): dot_product_self_attention function. \n",
    "        compute_attention_output_closure (function): Closure around compute_attention_output. \n",
    "        mode (str): 'train' or 'eval'.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: Multi-headed self-attention model.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert d_feature % n_heads == 0\n",
    "    d_head = d_feature // n_heads\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # (REPLACE INSTANCES OF 'None' WITH YOUR CODE)\n",
    "    \n",
    "    # HINT: The second argument to tl.Fn() is an uncalled function (without the parentheses)\n",
    "    # Since you are dealing with closures you might need to call the outer \n",
    "    # function with the correct parameters to get the actual uncalled function.\n",
    "    ComputeAttentionHeads = tl.Fn('AttnHeads', compute_attention_heads_closure(n_heads, d_head), n_out=1)\n",
    "        \n",
    "\n",
    "    return tl.Serial(\n",
    "        tl.Branch( # creates three towers for one input, takes activations and creates queries keys and values\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # queries\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # keys\n",
    "            [tl.Dense(d_feature), ComputeAttentionHeads], # values\n",
    "        ),\n",
    "        \n",
    "        tl.Fn('DotProductAttn', dot_product_self_attention, n_out=1), # takes QKV\n",
    "        # HINT: The second argument to tl.Fn() is an uncalled function\n",
    "        # Since you are dealing with closures you might need to call the outer \n",
    "        # function with the correct parameters to get the actual uncalled function.\n",
    "        tl.Fn('AttnOutput', compute_attention_output_closure(n_heads, d_head), n_out=1), # to allow for parallel\n",
    "        tl.Dense(d_feature)\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce756ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Branch_out3[\n",
      "    [Dense_512, AttnHeads]\n",
      "    [Dense_512, AttnHeads]\n",
      "    [Dense_512, AttnHeads]\n",
      "  ]\n",
      "  DotProductAttn_in3\n",
      "  AttnOutput\n",
      "  Dense_512\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the causal attention model\n",
    "print(CausalAttention(d_feature=512, n_heads=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a089a0ca",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "Serial[\n",
    "  Branch_out3[\n",
    "    [Dense_512, AttnHeads]\n",
    "    [Dense_512, AttnHeads]\n",
    "    [Dense_512, AttnHeads]\n",
    "  ]\n",
    "  DotProductAttn_in3\n",
    "  AttnOutput\n",
    "  Dense_512\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f8461bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "# test CausalAttention\n",
    "w2_tests.test_CausalAttention(CausalAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ca629",
   "metadata": {
    "colab_type": "text",
    "id": "W6zwtPjqRWRJ"
   },
   "source": [
    "<a name='2.3'></a>\n",
    "\n",
    "## 2.3 Transformer decoder block\n",
    "\n",
    "Now that you have implemented the causal part of the transformer, you will implement the transformer decoder block. Concretely you will be implementing this image now.\n",
    "\n",
    "<img src = \"images/transformer_decoder_1.png\" style = \"height:300px\"> \n",
    "\n",
    "To implement this function, you will have to call the `CausalAttention` or Masked multi-head attention function you implemented above. You will have to add a feedforward which consists of: \n",
    "\n",
    "- <span style='color:blue'> [tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) </span>: used to layer normalize\n",
    "- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: the dense layer\n",
    "- <span style='color:blue'> [ff_activation](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.activation_fns.Relu) </span>: feed forward activation (we use ReLu) here.\n",
    "- <span style='color:blue'> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) </span>: dropout layer\n",
    "- <span style='color:blue'> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: dense layer\n",
    "- <span style='color:blue'> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout) </span>: dropout layer\n",
    "\n",
    "Finally once you implement the feedforward, you can go ahead and implement the entire block using: \n",
    "\n",
    "- <span style='color:blue'> [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) </span>: takes in the tl.LayerNorm(), causal attention block, tl.dropout. \n",
    "\n",
    "- <span style='color:blue'> [tl.Residual](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Residual) </span>: takes in the feedforward block you will implement. \n",
    "\n",
    "<a name='ex03'></a>\n",
    "### Exercise 03\n",
    "**Instructions:** Implement the transformer decoder block. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bad231bf",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKOxnRbp1K5U"
   },
   "outputs": [],
   "source": [
    "# UNQ_C6\n",
    "# GRADED FUNCTION: DecoderBlock\n",
    "def DecoderBlock(d_model, d_ff, n_heads,\n",
    "                 dropout, mode, ff_activation):\n",
    "    \"\"\"Returns a list of layers that implements a Transformer decoder block.\n",
    "\n",
    "    The input is an activation tensor.\n",
    "\n",
    "    Args:\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        mode (str): 'train' or 'eval'.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        list: list of trax.layers.combinators.Serial that maps an activation tensor to an activation tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "    # Create masked multi-head attention block using CausalAttention function\n",
    "    causal_attention = CausalAttention( \n",
    "                        d_model,\n",
    "                        n_heads=n_heads,\n",
    "                        mode=mode\n",
    "                        )\n",
    "\n",
    "    # Create feed-forward block (list) with two dense layers with dropout and input normalized\n",
    "    feed_forward = [ \n",
    "        # Normalize layer inputs\n",
    "        tl.normalization.LayerNorm(),\n",
    "        # Add first feed forward (dense) layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(n_units=d_ff),\n",
    "        # Add activation function passed in as a parameter (you need to call it!)\n",
    "        ff_activation(), # Generally ReLU\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add second feed forward layer (don't forget to set the correct value for n_units)\n",
    "        tl.Dense(n_units=d_model),\n",
    "        # Add dropout with rate and mode specified (i.e., don't use dropout during evaluation)\n",
    "        tl.Dropout(rate=dropout, mode=mode)\n",
    "    ]\n",
    "\n",
    "    # Add list of two Residual blocks: the attention with normalization and dropout and feed-forward blocks\n",
    "    return [\n",
    "      tl.Residual(\n",
    "          # Normalize layer input\n",
    "          tl.normalization.LayerNorm(),\n",
    "          # Add causal attention block previously defined (without parentheses)\n",
    "          causal_attention,\n",
    "          # Add dropout with rate and mode specified\n",
    "          tl.Dropout(rate=dropout, mode=mode)\n",
    "        ),\n",
    "      tl.Residual(\n",
    "          # Add feed forward block (without parentheses)\n",
    "          feed_forward\n",
    "        ),\n",
    "      ]\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7a857d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Serial[\n",
      "  Branch_out2[\n",
      "    None\n",
      "    Serial[\n",
      "      LayerNorm\n",
      "      Serial[\n",
      "        Branch_out3[\n",
      "          [Dense_512, AttnHeads]\n",
      "          [Dense_512, AttnHeads]\n",
      "          [Dense_512, AttnHeads]\n",
      "        ]\n",
      "        DotProductAttn_in3\n",
      "        AttnOutput\n",
      "        Dense_512\n",
      "      ]\n",
      "      Dropout\n",
      "    ]\n",
      "  ]\n",
      "  Add_in2\n",
      "], Serial[\n",
      "  Branch_out2[\n",
      "    None\n",
      "    Serial[\n",
      "      LayerNorm\n",
      "      Dense_2048\n",
      "      Serial[\n",
      "        Relu\n",
      "      ]\n",
      "      Dropout\n",
      "      Dense_512\n",
      "      Dropout\n",
      "    ]\n",
      "  ]\n",
      "  Add_in2\n",
      "]]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the decoder block\n",
    "print(DecoderBlock(d_model=512, d_ff=2048, n_heads=8, dropout=0.1, mode='train', ff_activation=tl.Relu))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08839df5",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "[Serial[\n",
    "  Branch_out2[\n",
    "    None\n",
    "    Serial[\n",
    "      LayerNorm\n",
    "      Serial[\n",
    "        Branch_out3[\n",
    "          [Dense_512, AttnHeads]\n",
    "          [Dense_512, AttnHeads]\n",
    "          [Dense_512, AttnHeads]\n",
    "        ]\n",
    "        DotProductAttn_in3\n",
    "        AttnOutput\n",
    "        Dense_512\n",
    "      ]\n",
    "      Dropout\n",
    "    ]\n",
    "  ]\n",
    "  Add_in2\n",
    "], Serial[\n",
    "  Branch_out2[\n",
    "    None\n",
    "    Serial[\n",
    "      LayerNorm\n",
    "      Dense_2048\n",
    "      Serial[\n",
    "        Relu\n",
    "      ]\n",
    "      Dropout\n",
    "      Dense_512\n",
    "      Dropout\n",
    "    ]\n",
    "  ]\n",
    "  Add_in2\n",
    "]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1cbb7ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "# test DecoderBlock\n",
    "w2_tests.test_DecoderBlock(DecoderBlock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a4c46",
   "metadata": {
    "colab_type": "text",
    "id": "SoFv-nfLRWRN",
    "lines_to_next_cell": 0
   },
   "source": [
    "<a name='2.4'></a>\n",
    "## 2.4 Transformer Language Model\n",
    "\n",
    "You will now bring it all together. In this part you will use all the subcomponents you previously built to make the final model. Concretely, here is the image you will be implementing. \n",
    "<img src = \"images/transformer_decoder.png\" style = \"height:400px\">\n",
    "\n",
    "    \n",
    "<a name='ex04'></a>\n",
    "### Exercise 04\n",
    "**Instructions:** Previously you coded the decoder block. Now you will code the transformer language model. Here is what you will need. \n",
    "\n",
    "- <span style=\"color:blue\"> positional_enconder </span>- a list containing the following layers:\n",
    "    - <span style=\"color:blue\"> [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n",
    "    - <span style=\"color:blue\"> [tl.Dropout](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dropout)\n",
    "    - <span style=\"color:blue\"> [tl.PositionalEncoding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.PositionalEncoding)\n",
    "\n",
    "- A list of `n_layers` <span style=\"color:blue\"> decoder blocks</span>.\n",
    "- <span style=\"color:blue\"> [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial): </span> takes in the following layers or lists of layers:\n",
    "    - <span style=\"color:blue\"> [tl.ShiftRight](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.attention.ShiftRight): </span>: shift the tensor to the right by padding on axis 1.\n",
    "    - <span style=\"color:blue\"> positional_encoder </span>: encodes the text positions.\n",
    "    - <span style=\"color:blue\"> decoder_blocks </span>: the ones you created.\n",
    "    - <span style=\"color:blue\"> [tl.LayerNorm](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.normalization.LayerNorm) </span>: a layer norm.\n",
    "    - <span style=\"color:blue\"> [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense) </span>: takes in the vocab_size.\n",
    "    - <span style=\"color:blue\"> [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax) </span>: to predict.\n",
    "    \n",
    "Go go go!! You can do it :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f344edb7",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0yi4LJO1RWRS"
   },
   "outputs": [],
   "source": [
    "# UNQ_C7\n",
    "# GRADED FUNCTION: TransformerLM\n",
    "def TransformerLM(vocab_size=33300,\n",
    "                  d_model=512,\n",
    "                  d_ff=2048,\n",
    "                  n_layers=6,\n",
    "                  n_heads=8,\n",
    "                  dropout=0.1,\n",
    "                  max_len=4096,\n",
    "                  mode='train',\n",
    "                  ff_activation=tl.Relu):\n",
    "    \"\"\"Returns a Transformer language model.\n",
    "\n",
    "    The input to the model is a tensor of tokens. (This model uses only the\n",
    "    decoder part of the overall Transformer.)\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): vocab size.\n",
    "        d_model (int):  depth of embedding.\n",
    "        d_ff (int): depth of feed-forward layer.\n",
    "        n_layers (int): number of decoder layers.\n",
    "        n_heads (int): number of attention heads.\n",
    "        dropout (float): dropout rate (how much to drop out).\n",
    "        max_len (int): maximum symbol length for positional encoding.\n",
    "        mode (str): 'train', 'eval' or 'predict', predict mode is for fast inference.\n",
    "        ff_activation (function): the non-linearity in feed-forward layer.\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A Transformer language model as a layer that maps from a tensor of tokens\n",
    "        to activations over a vocab set.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "    # Embedding inputs and positional encoder\n",
    "    positional_encoder = [ \n",
    "        # Add embedding layer of dimension (vocab_size, d_model)\n",
    "        tl.Embedding(vocab_size, d_model),\n",
    "        # Use dropout with rate and mode specified\n",
    "        tl.Dropout(rate=dropout, mode=mode),\n",
    "        # Add positional encoding layer with maximum input length and mode specified\n",
    "        tl.PositionalEncoding(max_len=max_len, mode=mode)]\n",
    "\n",
    "    # Create stack (list) of decoder blocks with n_layers with necessary parameters\n",
    "    decoder_blocks = [ \n",
    "        DecoderBlock(d_model, d_ff, n_heads, dropout, mode, ff_activation) for _ in range(n_layers)]\n",
    "\n",
    "    # Create the complete model as written in the figure\n",
    "    return tl.Serial(\n",
    "        # Use teacher forcing (feed output of previous step to current step)\n",
    "        tl.ShiftRight(mode=mode), # Specify the mode!\n",
    "        # Add positional encoder\n",
    "        positional_encoder,\n",
    "        # Add decoder blocks\n",
    "        decoder_blocks,\n",
    "        # Normalize layer\n",
    "        tl.LayerNorm(),\n",
    "\n",
    "        # Add dense layer of vocab_size (since need to select a word to translate to)\n",
    "        # (a.k.a., logits layer. Note: activation already set by ff_activation)\n",
    "        tl.Dense(n_units=vocab_size),\n",
    "        # Get probabilities with Logsoftmax\n",
    "        tl.LogSoftmax()\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f49da557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial[\n",
      "  Serial[\n",
      "    ShiftRight(1)\n",
      "  ]\n",
      "  Embedding_33300_512\n",
      "  Dropout\n",
      "  PositionalEncoding\n",
      "  Serial[\n",
      "    Branch_out2[\n",
      "      None\n",
      "      Serial[\n",
      "        LayerNorm\n",
      "        Serial[\n",
      "          Branch_out3[\n",
      "            [Dense_512, AttnHeads]\n",
      "            [Dense_512, AttnHeads]\n",
      "            [Dense_512, AttnHeads]\n",
      "          ]\n",
      "          DotProductAttn_in3\n",
      "          AttnOutput\n",
      "          Dense_512\n",
      "        ]\n",
      "        Dropout\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  Serial[\n",
      "    Branch_out2[\n",
      "      None\n",
      "      Serial[\n",
      "        LayerNorm\n",
      "        Dense_2048\n",
      "        Serial[\n",
      "          Relu\n",
      "        ]\n",
      "        Dropout\n",
      "        Dense_512\n",
      "        Dropout\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  LayerNorm\n",
      "  Dense_33300\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the Transformer\n",
    "print(TransformerLM(n_layers=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94fd5fc",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "Serial[\n",
    "  Serial[\n",
    "    ShiftRight(1)\n",
    "  ]\n",
    "  Embedding_33300_512\n",
    "  Dropout\n",
    "  PositionalEncoding\n",
    "  Serial[\n",
    "    Branch_out2[\n",
    "      None\n",
    "      Serial[\n",
    "        LayerNorm\n",
    "        Serial[\n",
    "          Branch_out3[\n",
    "            [Dense_512, AttnHeads]\n",
    "            [Dense_512, AttnHeads]\n",
    "            [Dense_512, AttnHeads]\n",
    "          ]\n",
    "          DotProductAttn_in3\n",
    "          AttnOutput\n",
    "          Dense_512\n",
    "        ]\n",
    "        Dropout\n",
    "      ]\n",
    "    ]\n",
    "    Add_in2\n",
    "  ]\n",
    "  Serial[\n",
    "    Branch_out2[\n",
    "      None\n",
    "      Serial[\n",
    "        LayerNorm\n",
    "        Dense_2048\n",
    "        Serial[\n",
    "          Relu\n",
    "        ]\n",
    "        Dropout\n",
    "        Dense_512\n",
    "        Dropout\n",
    "      ]\n",
    "    ]\n",
    "    Add_in2\n",
    "  ]\n",
    "  LayerNorm\n",
    "  Dense_33300\n",
    "  LogSoftmax\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "33fb14f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "# test TransformerLM\n",
    "w2_tests.test_TransformerLM(TransformerLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169481c",
   "metadata": {
    "colab_type": "text",
    "id": "dRRKnoAdvmJ7"
   },
   "source": [
    "<a name='3'></a>\n",
    "# Part 3: Training\n",
    "\n",
    "Now you are going to train your model. As usual, you have to define the cost function, the optimizer, and decide whether you will be training it on a `gpu` or `cpu`. In this case, you will train your model on a cpu for a few steps and we will load in a pre-trained model that you can use to predict with your own words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bbab9",
   "metadata": {
    "colab_type": "text",
    "id": "l1lkVebQRWRV"
   },
   "source": [
    "<a name='3.1'></a>\n",
    "### 3.1 Training the model\n",
    "\n",
    "You will now write a function that takes in your model and trains it. To train your model you have to decide how many times you want to iterate over the entire data set. Each iteration is defined as an `epoch`. For each epoch, you have to go over all the data, using your training iterator.\n",
    "\n",
    "<a name='ex05'></a>\n",
    "### Exercise 05\n",
    "**Instructions:** Implement the `train_model` program below to train the neural network above. Here is a list of things you should do:\n",
    "\n",
    "- Create the train task by calling [`trax.supervised.training.TrainTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) and pass in the following: \n",
    "    - <span style='color:blue'> labeled_data </span> = train_gen\n",
    "    - <span style='color:blue'> loss_fn </span> = [tl.CrossEntropyLoss()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss)\n",
    "    - <span style='color:blue'> optimizer </span> = [trax.optimizers.Adam(0.01)](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam)\n",
    "    - <span style='color:blue'> lr_schedule </span> = [lr_schedule](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.lr_schedules.warmup_and_rsqrt_decay)\n",
    "\n",
    "\n",
    "- Create the eval task by calling [`trax.supervised.training.EvalTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) and pass in the following: \n",
    "    - <span style='color:blue'> labeled_data </span> = eval_gen\n",
    "    - <span style='color:blue'> metrics </span> = tl.CrossEntropyLoss() and [tl.Accuracy()](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy)\n",
    "    \n",
    "    \n",
    "- Create the training loop by calling [`trax.supervised.Training.Loop`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) and pass in the following: \n",
    "    - <span style='color:blue'> TransformerLM </span> \n",
    "    - <span style='color:blue'> train_task </span> \n",
    "    - <span style='color:blue'> eval_task </span> = [eval_task]\n",
    "    - <span style='color:blue'> output_dir</span> = output_dir\n",
    "    \n",
    "You will be using a cross entropy loss, with Adam optimizer. Please read the [Trax](https://trax-ml.readthedocs.io/en/latest/index.html) documentation to get a full understanding. \n",
    "\n",
    "The training loop that this function returns can be runned using the `run()` method by passing in the desired number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "20aa8326",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gM2gpu4xvjtX"
   },
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "# UNQ_C8\n",
    "# GRADED FUNCTION: train_model\n",
    "def training_loop(TransformerLM, train_gen, eval_gen, output_dir = \"~/model\"):\n",
    "    '''\n",
    "    Input:\n",
    "        TransformerLM (trax.layers.combinators.Serial): The model you are building.\n",
    "        train_gen (generator): Training stream of data.\n",
    "        eval_gen (generator): Evaluation stream of data.\n",
    "        output_dir (str): folder to save your file.\n",
    "        \n",
    "    Returns:\n",
    "        trax.supervised.training.Loop: Training loop.\n",
    "    '''\n",
    "    output_dir = os.path.expanduser(output_dir)  # trainer is an object\n",
    "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(n_warmup_steps=1000, max_value=0.01)\n",
    "\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    train_task = training.TrainTask( \n",
    "      labeled_data=train_gen, # The training generator\n",
    "      loss_layer=tl.CrossEntropyLoss(), # Loss function (Don't forget to instantiate!)\n",
    "      optimizer=trax.optimizers.Adam(0.01), # Optimizer (Don't forget to set LR to 0.01)\n",
    "      lr_schedule=lr_schedule,\n",
    "      n_steps_per_checkpoint=10 \n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask( \n",
    "      labeled_data=eval_gen, # The evaluation generator\n",
    "      metrics=[tl.CrossEntropyLoss(), tl.Accuracy()] # CrossEntropyLoss and Accuracy (Don't forget to instantiate both!)\n",
    "    )\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loop = training.Loop(TransformerLM(d_model=4,\n",
    "                                       d_ff=16,\n",
    "                                       n_layers=1,\n",
    "                                       n_heads=2,\n",
    "                                       mode='train'),\n",
    "                         train_task,\n",
    "                         eval_tasks=[eval_task],\n",
    "                         output_dir=output_dir)\n",
    "    \n",
    "    return loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58db914",
   "metadata": {},
   "source": [
    "Notice that the model will be trained for only 10 steps. \n",
    "\n",
    "Even with this constraint the model with the original default arguments took a very long time to finish. Because of this some parameters are changed when defining the model that is fed into the training loop in the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5de29ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "# test training_loop\n",
    "w2_tests.test_training_loop(training_loop, TransformerLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b89a3e31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "BFRBTwSqRWRZ",
    "outputId": "aff859e5-8f4a-4d3b-f1d3-98e137581a77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Total number of trainable weights: 316336\n",
      "Step      1: Ran 1 train steps in 8.51 secs\n",
      "Step      1: train CrossEntropyLoss |  10.40926647\n",
      "Step      1: eval  CrossEntropyLoss |  10.41445923\n",
      "Step      1: eval          Accuracy |  0.00000000\n",
      "\n",
      "Step     10: Ran 9 train steps in 72.12 secs\n",
      "Step     10: train CrossEntropyLoss |  10.41389942\n",
      "Step     10: eval  CrossEntropyLoss |  10.41506481\n",
      "Step     10: eval          Accuracy |  0.00000000\n"
     ]
    }
   ],
   "source": [
    "# Should take around 1.5 minutes\n",
    "!rm -f ~/model/model.pkl.gz\n",
    "loop = training_loop(TransformerLM, train_batch_stream, eval_batch_stream)\n",
    "loop.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8897d",
   "metadata": {
    "colab_type": "text",
    "id": "XKrEBjmskeWa"
   },
   "source": [
    " <a name='4'></a>\n",
    " # Part 4:  Evaluation  \n",
    "\n",
    "<a name='4.1'></a>\n",
    "### 4.1 Loading in a trained model\n",
    "\n",
    "In this part you will evaluate by loading in an almost exact version of the model you coded, but we trained it for you to save you time. Please run the cell below to load in the model.\n",
    "\n",
    "As you may have already noticed the model that you trained and the pretrained model share the same overall architecture but they have different values for some of the parameters:\n",
    "\n",
    "    \n",
    "   `Original (pretrained) model: `                                 \n",
    "                                       \n",
    "    TransformerLM(vocab_size=33300, d_model=512, d_ff=2048, n_layers=6, n_heads=8, \n",
    "                   dropout=0.1, max_len=4096, ff_activation=tl.Relu)\n",
    "                   \n",
    "   `Your model:`\n",
    "   \n",
    "    TransformerLM(d_model=4, d_ff=16, n_layers=1, n_heads=2)\n",
    "   \n",
    "   **Only the parameters shown for your model were changed. The others stayed the same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "996af87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS STEP COULD TAKE BETWEEN 15 SECONDS TO 15 MINUTES\n",
    "# Get the model architecture\n",
    "model = TransformerLM(mode='eval')\n",
    "\n",
    "# Load the pre-trained weights\n",
    "model.init_from_file('model.pkl.gz', weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a54f67",
   "metadata": {
    "colab_type": "text",
    "id": "ilM9C8P3RWRf"
   },
   "source": [
    "<a name='5'></a>\n",
    "# Part 5: Testing with your own input\n",
    "\n",
    "You will now test your input. You are going to implement greedy decoding. This consists of two functions. The first one allows you to identify the next symbol. It gets the argmax of the output of your model and then returns that index. \n",
    "\n",
    "<a name='ex06'></a>\n",
    "### Exercise 06\n",
    "**Instructions:** Implement the next symbol function that takes in the cur_output_tokens and the trained model to return the the index of the next word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cafa6423",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD_bXRCpRWRg"
   },
   "outputs": [],
   "source": [
    "# UNQ_C9\n",
    "def next_symbol(cur_output_tokens, model):\n",
    "    \"\"\"Returns the next symbol for a given sentence.\n",
    "\n",
    "    Args:\n",
    "        cur_output_tokens (list): tokenized sentence with EOS and PAD tokens at the end.\n",
    "        model (trax.layers.combinators.Serial): The transformer model.\n",
    "\n",
    "    Returns:\n",
    "        int: tokenized symbol.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    \n",
    "    # current output tokens length\n",
    "    token_length = len(cur_output_tokens)\n",
    "    # calculate the minimum power of 2 big enough to store token_length\n",
    "    # HINT: use np.ceil() and np.log2()\n",
    "    # add 1 to token_length so np.log2() doesn't receive 0 when token_length is 0\n",
    "    padded_length = 2**int(np.ceil(np.log2(token_length + 1)))\n",
    "\n",
    "    # Fill cur_output_tokens with 0's until it reaches padded_length\n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    padded_with_batch = np.array(padded)[None, :] # Don't replace this None! This is a way of setting the batch dim\n",
    "\n",
    "    # model expects a tuple containing two padded tensors (with batch)\n",
    "    output, _ = model((padded_with_batch, padded_with_batch)) \n",
    "    # HINT: output has shape (1, padded_length, vocab_size)\n",
    "    # To get log_probs you need to index output wih 0 in the first dim\n",
    "    # token_length in the second dim and all of the entries for the last dim.\n",
    "    log_probs = output[0, token_length, :]\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return int(np.argmax(log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "10dcf534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it out!\n",
    "sentence_test_nxt_symbl = \"I want to fly in the sky.\"\n",
    "detokenize([next_symbol(tokenize(sentence_test_nxt_symbl)+[0], model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8189e2ac",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "'The'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "642e24cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "# test next_symbol\n",
    "w2_tests.test_next_symbol(next_symbol, TransformerLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa962a",
   "metadata": {
    "colab_type": "text",
    "id": "2AwrQFglRWRj"
   },
   "source": [
    "<a name='5.1'></a>\n",
    "### 5.1 Greedy decoding\n",
    "\n",
    "Now you will implement the greedy_decode algorithm that will call the `next_symbol` function. It takes in the input_sentence, the trained model and returns the the decoded sentence. \n",
    "\n",
    "<a name='ex07'></a>\n",
    "### Exercise 07\n",
    "\n",
    "**Instructions**: Implement the greedy_decode algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1d3c07cb",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HwIdimiN0k2"
   },
   "outputs": [],
   "source": [
    "# UNQ_C10\n",
    "# Decoding functions.\n",
    "def greedy_decode(input_sentence, model, next_symbol=next_symbol, tokenize=tokenize, detokenize=detokenize):\n",
    "    \"\"\"Greedy decode function.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (string): a sentence or article.\n",
    "        model (trax.layers.combinators.Serial): Transformer model.\n",
    "\n",
    "    Returns:\n",
    "        string: summary of the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
    "    # Use tokenize()\n",
    "    cur_output_tokens = tokenize(input_sentence) + [0]    \n",
    "    generated_output = [] \n",
    "    cur_output = 0 \n",
    "    EOS = 1 \n",
    "    \n",
    "    while cur_output != EOS:\n",
    "        # Get next symbol\n",
    "        cur_output = next_symbol(cur_output_tokens, model)\n",
    "        # Append next symbol to original sentence\n",
    "        cur_output_tokens.append(cur_output)\n",
    "        # Append next symbol to generated sentence\n",
    "        generated_output.append(cur_output)\n",
    "        \n",
    "        print(detokenize(generated_output))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return detokenize(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f39ddd96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "9kHuIDGW1sOr",
    "outputId": "2525ca2c-4625-47c0-8456-f75598581993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a sunny day when I went to the market to buy some flowers. But\n",
      "I only found roses, not tulips. \n",
      "\n",
      ":\n",
      ": I\n",
      ": I just\n",
      ": I just found\n",
      ": I just found ros\n",
      ": I just found roses\n",
      ": I just found roses,\n",
      ": I just found roses, not\n",
      ": I just found roses, not tu\n",
      ": I just found roses, not tulips\n",
      ": I just found roses, not tulips\n",
      ": I just found roses, not tulips.\n",
      ": I just found roses, not tulips.<EOS>\n",
      ": I just found roses, not tulips.<EOS>\n"
     ]
    }
   ],
   "source": [
    "# Test it out on a sentence!\n",
    "test_sentence = \"It was a sunny day when I went to the market to buy some flowers. But I only found roses, not tulips.\"\n",
    "print(wrapper.fill(test_sentence), '\\n')\n",
    "print(greedy_decode(test_sentence, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d5fe2",
   "metadata": {
    "colab_type": "text",
    "id": "CA-279WI2D3G"
   },
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    ":\n",
    ": I\n",
    ": I just\n",
    ": I just found\n",
    ": I just found ros\n",
    ": I just found roses\n",
    ": I just found roses,\n",
    ": I just found roses, not\n",
    ": I just found roses, not tu\n",
    ": I just found roses, not tulips\n",
    ": I just found roses, not tulips\n",
    ": I just found roses, not tulips.\n",
    ": I just found roses, not tulips.<EOS>\n",
    ": I just found roses, not tulips.<EOS>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a688825c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DYgX-mzjyUia",
    "outputId": "b901e164-48b3-4124-d21a-fe7443d15b79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It’s the posing craze sweeping the U.S. after being brought to fame by\n",
      "skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert\n",
      "Pujols - and even Republican politician Rick Perry. But now four\n",
      "students at Riverhead High School on Long Island, New York, have been\n",
      "suspended for dropping to a knee and taking up a prayer pose to mimic\n",
      "Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel,\n",
      "Tyler Carroll and Connor Carroll were all suspended for one day\n",
      "because the ‘Tebowing’ craze was blocking the hallway and presenting a\n",
      "safety hazard to students. Scroll down for video. Banned: Jordan\n",
      "Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured\n",
      "left) were all suspended for one day by Riverhead High School on Long\n",
      "Island, New York, for their tribute to Broncos quarterback Tim Tebow.\n",
      "Issue: Four of the pupils were suspended for one day because they\n",
      "allegedly did not heed to warnings that the 'Tebowing' craze at the\n",
      "school was blocking the hallway and presenting a safety hazard to\n",
      "students. \n",
      "\n",
      "Jordan\n",
      "Jordan Ful\n",
      "Jordan Fulcol\n",
      "Jordan Fulcoly\n",
      "Jordan Fulcoly,\n",
      "Jordan Fulcoly, Wayne\n",
      "Jordan Fulcoly, Wayne Dre\n",
      "Jordan Fulcoly, Wayne Drexe\n",
      "Jordan Fulcoly, Wayne Drexel\n",
      "Jordan Fulcoly, Wayne Drexel,\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day.\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not hee\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warn\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the '\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Te\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebow\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "cra\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocki\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hall\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students.\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students.<EOS>\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students.<EOS>\n"
     ]
    }
   ],
   "source": [
    "# Test it out with a whole article!\n",
    "article = \"It’s the posing craze sweeping the U.S. after being brought to fame by skier Lindsey Vonn, soccer star Omar Cummings, baseball player Albert Pujols - and even Republican politician Rick Perry. But now four students at Riverhead High School on Long Island, New York, have been suspended for dropping to a knee and taking up a prayer pose to mimic Denver Broncos quarterback Tim Tebow. Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were all suspended for one day because the ‘Tebowing’ craze was blocking the hallway and presenting a safety hazard to students. Scroll down for video. Banned: Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll (all pictured left) were all suspended for one day by Riverhead High School on Long Island, New York, for their tribute to Broncos quarterback Tim Tebow. Issue: Four of the pupils were suspended for one day because they allegedly did not heed to warnings that the 'Tebowing' craze at the school was blocking the hallway and presenting a safety hazard to students.\"\n",
    "print(wrapper.fill(article), '\\n')\n",
    "print(greedy_decode(article, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e1650",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "Jordan\n",
    "Jordan Ful\n",
    "Jordan Fulcol\n",
    "Jordan Fulcoly\n",
    "Jordan Fulcoly,\n",
    "Jordan Fulcoly, Wayne\n",
    "Jordan Fulcoly, Wayne Dre\n",
    "Jordan Fulcoly, Wayne Drexe\n",
    "Jordan Fulcoly, Wayne Drexel\n",
    "Jordan Fulcoly, Wayne Drexel,\n",
    ".\n",
    ".\n",
    ".\n",
    "\n",
    "Final summary:\n",
    "\n",
    "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
    "suspended for one day. Four students were suspended for one day\n",
    "because they allegedly did not heed to warnings that the 'Tebowing'\n",
    "craze was blocking the hallway and presenting a safety hazard to\n",
    "students.<EOS>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "51077871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      ": I\n",
      ": I just\n",
      ": I just found\n",
      ": I just found ros\n",
      ": I just found roses\n",
      ": I just found roses,\n",
      ": I just found roses, not\n",
      ": I just found roses, not tu\n",
      ": I just found roses, not tulips\n",
      ": I just found roses, not tulips\n",
      ": I just found roses, not tulips.\n",
      ": I just found roses, not tulips.<EOS>\n",
      "Jordan\n",
      "Jordan Ful\n",
      "Jordan Fulcol\n",
      "Jordan Fulcoly\n",
      "Jordan Fulcoly,\n",
      "Jordan Fulcoly, Wayne\n",
      "Jordan Fulcoly, Wayne Dre\n",
      "Jordan Fulcoly, Wayne Drexe\n",
      "Jordan Fulcoly, Wayne Drexel\n",
      "Jordan Fulcoly, Wayne Drexel,\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day.\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not hee\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warn\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the '\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Te\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebow\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "cra\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocki\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hall\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students.\n",
      "Jordan Fulcoly, Wayne Drexel, Tyler Carroll and Connor Carroll were\n",
      "suspended for one day. Four students were suspended for one day\n",
      "because they allegedly did not heed to warnings that the 'Tebowing'\n",
      "craze was blocking the hallway and presenting a safety hazard to\n",
      "students.<EOS>\n",
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# UNIT TEST\n",
    "# test greedy_decode\n",
    "w2_tests.test_greedy_decode(greedy_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc17f9",
   "metadata": {},
   "source": [
    "**Congratulations on finishing this week's assignment!** You did a lot of work and now you should have a better understanding of the enconder part of Transformers and how Transformers can be used for text summarization.\n",
    "\n",
    "**Keep it up!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
